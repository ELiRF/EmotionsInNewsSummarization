{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarizers Get Emotional on News Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "The objective is to analyze the capabilities of abstractive models to generate summaries that exhibit emotions similar to those a human would show in their summaries, and to attempt to improve them.\n",
    "\n",
    "The following figure shows a subset of sentences from an article about a plane crash in Canada. Some of these sentences are used in the reference summary. Which ones would you say?\n",
    "\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary><u><b>Click here to display the figure with the text</b></u></summary>\n",
    "<img src=\"https://iili.io/RGPLP4.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary><u><b>Click here to display the sentences that appear in the human summary</b></u></summary>\n",
    "<img src=\"https://iili.io/RGPsVf.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "Regarding the topic, we only found [[1]](https://www.cs.utoronto.ca/~akennedy/publications/summarization_cai_2012.pdf) (also [its slides](https://pdfs.semanticscholar.org/55da/aeff6d8684d65fb8981afe8e89a655f54f54.pdf)), which served us as a foundation (speech, some formulas, etc.).\n",
    "\n",
    "\n",
    "> News is not simply a straight re-telling of events, but rather an interpretation of those events by a reporter, whose feelings and opinions can often become part of the story itself. The emotion of a story is also important to its meaning.\n",
    "\n",
    "> The human summarizers appear to favour emotional content when generating summaries. It is difficult to explain precisely why this is so; perhaps some aspects of the queries are more naturally answered with emotion.\n",
    " \n",
    "We analyzed four aspects with two corpora (CNNDM and XSUM) and several models (**Abstractives**: BART, PEGASUS, T5, BART-JES y BART-JES-Oracle. **Extractives**: Lead, Random, Extractive oracle):\n",
    "\n",
    "* [**Emotional Coherence**](#coherenciaysesgo): Are the emotions in the generated summaries coherent with the emotions in the references?\n",
    "* [**Emotional Bias**](#coherenciaysesgo): Tendency of humans to [over/under]represent certain emotions in the summaries more than in the documents [[1]](https://www.cs.utoronto.ca/~akennedy/publications/summarization_cai_2012.pdf). Do the models exhibit a similar emotional bias to humans? i.e., do they reinforce [more/less] the emotions that humans reinforce [more/less] in the summaries than in the documents?\n",
    "* [**ROUGE Correlation**](#rougecorrel): How do coherence and emotional bias metrics correlate with ROUGE?\n",
    "\n",
    "* [**Emotions of novel words**](#alucinacion): Are the emotions of the novel words in the generated summaries coherent with the emotions in the references?\n",
    "\n",
    "Para todos los análisis nos hemos basado en la **Emotion Density** (ED) y la **Emotion Ratio** entre (referencia, artículo) o (generado, artículo) (las llamamos $R_{M/D}$ y $R_{G/D}$ respectivamente) [[1]](https://www.cs.utoronto.ca/~akennedy/publications/summarization_cai_2012.pdf):\n",
    "\n",
    "<img src=\"https://iili.io/RGirgV.png\" width=\"600\" height=\"200\"/>\n",
    "<br>\n",
    "<img src=\"https://iili.io/RGivWb.png\" width=\"600\" height=\"200\"/>\n",
    "\n",
    "\n",
    "The authors of [[1]](https://www.cs.utoronto.ca/~akennedy/publications/summarization_cai_2012.pdf) just use $ED_M$, $ED_D$ y $R_{M/D}$ for their analysis, but we need $ED_G$ and $R_{G/D}$ too (computed from the generated summaries instead from the references).\n",
    "\n",
    "To detect words with emotions/polarity, we use the lemmatized NRC lexicon and the [NRCLex](https://github.com/metalcorebear/NRCLex) library.\n",
    "\n",
    "Except for **Emotions of novel words** where we use Precision, Recall, and $F_1$, in the rest of the analysis, we use Pearson correlations.\n",
    "\n",
    "This notebook also contains ideas to try to [**improve the models**](#modelado) and several [**examples**](#visualizaciones). Regarding model improvement, we have proposed \"Joint Emotion and Summary generation\" (JES), as in [[2]](https://arxiv.org/pdf/2104.07606.pdf) and [[3]](https://arxiv.org/abs/2102.09130), but using prefixes with emotional words instead of entities. In the experimentation, we used BART to implement JES (BART-JES). The results of BART-JES are generally worse than BART-CNNDM in terms of emotions and ROUGE.\n",
    "However, it has the advantage of being controllable, i.e., we can control the emotions/content of the generated summaries using keywords that trigger those emotions (you have two examples [**here**](#controlled)). Additionally, by using good prompts to condition the summary, results can be increased significantly in all metrics (BART-JES-Oracle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "\n",
    "\n",
    "https://www.cs.utoronto.ca/~akennedy/publications/summarization_cai_2012.pdf\n",
    "\n",
    "https://link.springer.com/chapter/10.1007/978-81-322-2523-2_39\n",
    "\n",
    "https://link.springer.com/chapter/10.1007/978-981-13-0589-4_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jogonba2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jogonba2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jogonba2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jogonba2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jogonba2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jogonba2/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import spacy\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "from copy import copy\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from generation_hyperparameters import generation_hyperparameters\n",
    "from IPython.core.display import display, HTML\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jogonba2/anaconda3/lib/python3.7/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "emotion_labels = [\n",
    "    \"fear\",\n",
    "    \"anger\",\n",
    "    \"anticipation\",\n",
    "    \"trust\",\n",
    "    \"surprise\",\n",
    "    \"sadness\",\n",
    "    \"disgust\",\n",
    "    \"joy\",\n",
    "    \"none\",\n",
    "]\n",
    "\n",
    "sentiment_labels = [\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"none\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDistributionMetrics:\n",
    "    def compute_state_dict(dataset, labels):\n",
    "        \"\"\"\n",
    "        For each example in the dataset, calculate ED_M(E_i) [reference],\n",
    "        ED_D(E_i) [document], and ED_G(E_i) [generated] as described in\n",
    "        https://www.cs.utoronto.ca/~akennedy/publications/summarization_cai_2012.pdf.\n",
    "        \"\"\"\n",
    "        n_samples = len(dataset[\"nrclex_documents\"])\n",
    "        state_dict = {}\n",
    "        for key in [\"ed_m\", \"ed_d\", \"ed_g\", \"ratio_m-d\", \"ratio_g-d\"]:\n",
    "            state_dict[key] = [\n",
    "                {label: 0 for label in labels} for _ in range(n_samples)\n",
    "            ]\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            # Compute densities\n",
    "            for key1, key2 in [\n",
    "                (\"ed_m\", \"nrclex_ref_summaries\"),\n",
    "                (\"ed_d\", \"nrclex_documents\"),\n",
    "                (\"ed_g\", \"nrclex_gen_summaries\"),\n",
    "            ]:\n",
    "                # Fill counts of \"none\" (1)\n",
    "                state_dict[key1][i] = {\n",
    "                    **state_dict[key1][i],\n",
    "                    **{\n",
    "                        label: dataset[key2][i].raw_emotion_scores[label]\n",
    "                        for label in labels\n",
    "                        if label in dataset[key2][i].raw_emotion_scores\n",
    "                    },\n",
    "                }\n",
    "\n",
    "                # Fill counts of \"none\" (2)\n",
    "                n_total_words = len(dataset[key2][i].words)\n",
    "                n_words_with_label = len(\n",
    "                    [\n",
    "                        word\n",
    "                        for word in dataset[key2][i].affect_dict\n",
    "                        if len(\n",
    "                            set(labels).intersection(\n",
    "                                set(dataset[key2][i].affect_dict[word])\n",
    "                            )\n",
    "                        )\n",
    "                        >= 1\n",
    "                    ]\n",
    "                )\n",
    "                state_dict[key1][i][\"none\"] = n_total_words - n_words_with_label\n",
    "\n",
    "                # Normalize to compute ED\n",
    "                denom = sum([state_dict[key1][i][label] for label in labels])\n",
    "                state_dict[key1][i] = {\n",
    "                    label: label_freq / denom if denom > 0 else 0\n",
    "                    for label, label_freq in state_dict[key1][i].items()\n",
    "                }\n",
    "\n",
    "            # Compute R_{M/D} y R_{G/D}\n",
    "            for ratio, summ_key, doc_key in [\n",
    "                (\"ratio_m-d\", \"ed_m\", \"ed_d\"),\n",
    "                (\"ratio_g-d\", \"ed_g\", \"ed_d\"),\n",
    "            ]:\n",
    "                state_dict[ratio][i] = {\n",
    "                    label: state_dict[summ_key][i][label]\n",
    "                    / state_dict[doc_key][i][label]\n",
    "                    if state_dict[doc_key][i][label] > 0\n",
    "                    else 0\n",
    "                    for label in labels\n",
    "                }\n",
    "\n",
    "        return state_dict\n",
    "\n",
    "    def emotion_correlation(\n",
    "        state_dict, labels, func=\"pearson\", mode=\"densities\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate the Pearson or Spearman correlation between:\n",
    "        - Ratios: the ratios R_{M/D} and R{G/D} for each emotion.\n",
    "        - Densities: the densities E_M and E_G for each emotion\n",
    "        \"\"\"\n",
    "        corrs = {label: 0 for label in labels}\n",
    "\n",
    "        if mode == \"densities\":\n",
    "            key_ref = \"ed_m\"\n",
    "            key_gen = \"ed_g\"\n",
    "        elif mode == \"ratios\":\n",
    "            key_ref = \"ratio_m-d\"\n",
    "            key_gen = \"ratio_g-d\"\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        func = pearsonr if func == \"pearson\" else spearmanr\n",
    "        for label in labels:\n",
    "            ref_emo = []\n",
    "            gen_emo = []\n",
    "            for i in range(len(state_dict[key_ref])):\n",
    "                ref_emo.append(state_dict[key_ref][i][label])\n",
    "                gen_emo.append(state_dict[key_gen][i][label])\n",
    "            corrs[label] = func(ref_emo, gen_emo)\n",
    "\n",
    "        return corrs\n",
    "\n",
    "    def emotion_summary_metric_correlation(\n",
    "        state_dict,\n",
    "        dataset,\n",
    "        emo_metric_func=\"pearson\",\n",
    "        mode=\"densities\",\n",
    "        correlation_func=\"pearson\",\n",
    "        summary_metric=\"rouge\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate the Pearson or Spearman correlation between the emotional metric\n",
    "        (calculated with densities or ratios) and other summary evaluation metrics\n",
    "        (ROUGE->average of R1, R2, and RL (F1)) or BERTScore:\n",
    "        - Density: Pearson(correlation(ED_M[i], ED_G[i]), ROUGE[i])\n",
    "        - Ratio: Pearson(correlation(R{M/D}[i], R{M/G}[i]), ROUGE[i])\n",
    "        - emo_metric_func: correlation metric between ED_M[i] and ED_G[i] (or R{M/D}[i], R{M/G[i]})\n",
    "        - correlation_func: correlation function between the emotional metrics and ROUGE\n",
    "        \"\"\"\n",
    "\n",
    "        if mode == \"densities\":\n",
    "            key_ref = \"ed_m\"\n",
    "            key_gen = \"ed_g\"\n",
    "        elif mode == \"ratios\":\n",
    "            key_ref = \"ratio_m-d\"\n",
    "            key_gen = \"ratio_g-d\"\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        emo_metric_func = (\n",
    "            pearsonr if emo_metric_func == \"pearson\" else spearmanr\n",
    "        )\n",
    "        correlation_func = (\n",
    "            pearsonr if correlation_func == \"pearson\" else spearmanr\n",
    "        )\n",
    "\n",
    "        emo_scores = []\n",
    "        summary_scores = []\n",
    "\n",
    "        for i in range(len(state_dict[key_ref])):\n",
    "            emo_dist_ref = list(state_dict[key_ref][i].values())\n",
    "            emo_dist_gen = list(state_dict[key_gen][i].values())\n",
    "            emo_scores.append(emo_metric_func(emo_dist_ref, emo_dist_gen)[0])\n",
    "            if summary_metric == \"rouge\":\n",
    "                summary_scores.append(\n",
    "                    (\n",
    "                        dataset[\"rouge_scores\"][\"rouge1_f1\"][i]\n",
    "                        + dataset[\"rouge_scores\"][\"rouge2_f1\"][i]\n",
    "                        + dataset[\"rouge_scores\"][\"rougeLsum_f1\"][i]\n",
    "                    )\n",
    "                    / 3.0\n",
    "                )\n",
    "            else:\n",
    "                summary_scores.append(dataset[\"bert_scores\"][\"f1\"][i])\n",
    "\n",
    "        emo_scores = np.nan_to_num(emo_scores)\n",
    "        summary_scores = np.nan_to_num(summary_scores)\n",
    "\n",
    "        return correlation_func(emo_scores, summary_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionalHallucinationMetric:\n",
    "    \"\"\"\n",
    "    Metric to compute Emotions of novel words.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute(dataset, lemmatized=False):\n",
    "        outputs = {\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "        n_samples = len(dataset[\"documents\"])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            # Pick emotions in the reference summary.\n",
    "            ref_emotions = set(dataset[\"nrclex_ref_summaries\"][i].affect_list)\n",
    "\n",
    "            # Pick emotions of the novel words in the generated summary.\n",
    "            if not lemmatized:\n",
    "                novel_words = list(\n",
    "                    set(\n",
    "                        [\n",
    "                            word.lower()\n",
    "                            for word in dataset[\"nrclex_gen_summaries\"][\n",
    "                                i\n",
    "                            ].affect_dict.keys()\n",
    "                        ]\n",
    "                    ).difference(\n",
    "                        set(\n",
    "                            [\n",
    "                                word.lower()\n",
    "                                for word in dataset[\"nrclex_documents\"][i].words\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                novel_words = list(\n",
    "                    set(\n",
    "                        [\n",
    "                            word.lower()\n",
    "                            for word in word_tokenize(\n",
    "                                dataset[\"gen_summaries\"][i]\n",
    "                            )\n",
    "                        ]\n",
    "                    ).difference(\n",
    "                        set(\n",
    "                            [\n",
    "                                word.lower()\n",
    "                                for word in word_tokenize(\n",
    "                                    dataset[\"documents\"][i]\n",
    "                                )\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                lemmatized_novel_words = []\n",
    "                for novel_word in nlp.pipe(novel_words):\n",
    "                    lemmatized_novel_words.append(novel_word[0].lemma_)\n",
    "\n",
    "                novel_words = [\n",
    "                    novel_word\n",
    "                    for novel_word in set(lemmatized_novel_words)\n",
    "                    if novel_word\n",
    "                    in dataset[\"nrclex_gen_summaries\"][i].affect_dict\n",
    "                ]\n",
    "\n",
    "            novel_words_emotions = set(\n",
    "                sum(\n",
    "                    [\n",
    "                        dataset[\"nrclex_gen_summaries\"][i].affect_dict[word]\n",
    "                        for word in novel_words\n",
    "                    ],\n",
    "                    [],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # If there are no emotions in the reference, there are no\n",
    "            # novel words, or there are novel words w/o emotions, then\n",
    "            # discard the sample.\n",
    "            if len(ref_emotions) == 0 or len(novel_words_emotions) == 0:\n",
    "                continue\n",
    "\n",
    "            # Otherwise, compute P, R, and F1\n",
    "            else:\n",
    "                precision = len(\n",
    "                    novel_words_emotions.intersection(ref_emotions)\n",
    "                ) / len(novel_words_emotions)\n",
    "                recall = len(\n",
    "                    novel_words_emotions.intersection(ref_emotions)\n",
    "                ) / len(ref_emotions)\n",
    "                f1 = (\n",
    "                    (2 * precision * recall) / (precision + recall)\n",
    "                    if (precision + recall) > 0\n",
    "                    else 0\n",
    "                )\n",
    "                outputs[\"precision\"].append(precision)\n",
    "                outputs[\"recall\"].append(recall)\n",
    "                outputs[\"f1\"].append(f1)\n",
    "\n",
    "        print(\"Evaluating with %d samples\" % len(outputs[\"precision\"]))\n",
    "\n",
    "        outputs[\"precision\"] = np.array(outputs[\"precision\"])\n",
    "        outputs[\"recall\"] = np.array(outputs[\"recall\"])\n",
    "        outputs[\"f1\"] = np.array(outputs[\"f1\"])\n",
    "\n",
    "        n_samples = len(outputs[\"precision\"])\n",
    "        conf_interval_95_f = lambda std: 1.96 * (std / np.sqrt(n_samples))\n",
    "\n",
    "        outputs[\"precision\"] = (\n",
    "            outputs[\"precision\"].mean(),\n",
    "            outputs[\"precision\"].std(),\n",
    "            conf_interval_95_f(outputs[\"precision\"].std()),\n",
    "        )\n",
    "        outputs[\"recall\"] = (\n",
    "            outputs[\"recall\"].mean(),\n",
    "            outputs[\"recall\"].std(),\n",
    "            conf_interval_95_f(outputs[\"recall\"].std()),\n",
    "        )\n",
    "        outputs[\"f1\"] = (\n",
    "            outputs[\"f1\"].mean(),\n",
    "            outputs[\"f1\"].std(),\n",
    "            conf_interval_95_f(outputs[\"f1\"].std()),\n",
    "        )\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualization:\n",
    "    def get_top_k_examples_emotional_hallucination(\n",
    "        dataset,\n",
    "        k=10,\n",
    "        reverse=False,\n",
    "        sort_metric=\"precision\",\n",
    "        lemmatized=False,\n",
    "    ):\n",
    "        outputs = {\n",
    "            \"documents\": [],\n",
    "            \"ref_summaries\": [],\n",
    "            \"gen_summaries\": [],\n",
    "            \"nrclex_documents\": [],\n",
    "            \"nrclex_ref_summaries\": [],\n",
    "            \"nrclex_gen_summaries\": [],\n",
    "            \"novel_words_with_emotions\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": [],\n",
    "        }\n",
    "        n_samples = len(dataset[\"documents\"])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            # Pick emotions from the reference summary.\n",
    "            ref_emotions = set(dataset[\"nrclex_ref_summaries\"][i].affect_list)\n",
    "\n",
    "            # Pick emotions of the novel words in the generated summary.\n",
    "            if not lemmatized:\n",
    "                novel_words = list(\n",
    "                    set(\n",
    "                        [\n",
    "                            word.lower()\n",
    "                            for word in dataset[\"nrclex_gen_summaries\"][\n",
    "                                i\n",
    "                            ].affect_dict.keys()\n",
    "                        ]\n",
    "                    ).difference(\n",
    "                        set(\n",
    "                            [\n",
    "                                word.lower()\n",
    "                                for word in dataset[\"nrclex_documents\"][i].words\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                novel_words = list(\n",
    "                    set(\n",
    "                        [\n",
    "                            word.lower()\n",
    "                            for word in word_tokenize(dataset[\"gen_summaries\"][i])\n",
    "                        ]\n",
    "                    ).difference(\n",
    "                        set(\n",
    "                            [\n",
    "                                word.lower()\n",
    "                                for word in word_tokenize(dataset[\"documents\"][i])\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                lemmatized_novel_words = []\n",
    "                for novel_word in nlp.pipe(novel_words):\n",
    "                    lemmatized_novel_words.append(novel_word[0].lemma_)\n",
    "\n",
    "                novel_words = [\n",
    "                    novel_word\n",
    "                    for novel_word in set(lemmatized_novel_words)\n",
    "                    if novel_word in dataset[\"nrclex_gen_summaries\"][i].affect_dict\n",
    "                ]\n",
    "\n",
    "            novel_words_emotions = set(\n",
    "                sum(\n",
    "                    [\n",
    "                        dataset[\"nrclex_gen_summaries\"][i].affect_dict[word]\n",
    "                        for word in novel_words\n",
    "                    ],\n",
    "                    [],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # If there are no emotions in the reference, there are no\n",
    "            # novel words, or there are novel words w/o emotions, then\n",
    "            # discard the sample.\n",
    "            if len(ref_emotions) == 0 or len(novel_words_emotions) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Otherwise, compute P, R, and F1\n",
    "            else:\n",
    "                precision = len(novel_words_emotions.intersection(ref_emotions)) / len(\n",
    "                    novel_words_emotions\n",
    "                )\n",
    "                recall = len(novel_words_emotions.intersection(ref_emotions)) / len(\n",
    "                    ref_emotions\n",
    "                )\n",
    "                f1 = (\n",
    "                    (2 * precision * recall) / (precision + recall)\n",
    "                    if (precision + recall) > 0\n",
    "                    else 0\n",
    "                )\n",
    "\n",
    "                outputs[\"precision\"].append(precision)\n",
    "                outputs[\"recall\"].append(recall)\n",
    "                outputs[\"f1\"].append(f1)\n",
    "                for key in [\n",
    "                    \"documents\",\n",
    "                    \"ref_summaries\",\n",
    "                    \"gen_summaries\",\n",
    "                    \"nrclex_documents\",\n",
    "                    \"nrclex_ref_summaries\",\n",
    "                    \"nrclex_gen_summaries\",\n",
    "                ]:\n",
    "                    outputs[key].append(dataset[key][i])\n",
    "\n",
    "                outputs[\"novel_words_with_emotions\"].append(\n",
    "                    list(\n",
    "                        zip(\n",
    "                            novel_words,\n",
    "                            [\n",
    "                                dataset[\"nrclex_gen_summaries\"][i].affect_dict[word]\n",
    "                                for word in novel_words\n",
    "                            ],\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for key in outputs:\n",
    "            outputs[key] = np.array(outputs[key])\n",
    "\n",
    "        sorted_ids = outputs[sort_metric].argsort()\n",
    "\n",
    "        if reverse:\n",
    "            sorted_ids = sorted_ids[::-1]\n",
    "\n",
    "        for key in outputs:\n",
    "            outputs[key] = outputs[key][sorted_ids][:k]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statistics:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.n_samples = len(dataset[\"nrclex_documents\"])\n",
    "        self.stats = {\n",
    "            \"percentage_documents_with_emotion\": {\n",
    "                emotion: 0.0 for emotion in emotion_labels\n",
    "            },\n",
    "            \"percentage_documents_with_sentiment\": {\n",
    "                sentiment: 0.0 for sentiment in sentiment_labels\n",
    "            },\n",
    "            \"percentage_summaries_with_emotion\": {\n",
    "                emotion: 0.0 for emotion in emotion_labels\n",
    "            },\n",
    "            \"percentage_summaries_with_sentiment\": {\n",
    "                sentiment: 0.0 for sentiment in sentiment_labels\n",
    "            },\n",
    "            \"average_document_emotion_density\": {\n",
    "                emotion: 0.0 for emotion in emotion_labels\n",
    "            },\n",
    "            \"average_document_sentiment_density\": {\n",
    "                sentiment: 0.0 for sentiment in sentiment_labels\n",
    "            },\n",
    "            \"average_summary_emotion_density\": {\n",
    "                emotion: 0.0 for emotion in emotion_labels\n",
    "            },\n",
    "            \"average_summary_sentiment_density\": {\n",
    "                sentiment: 0.0 for sentiment in sentiment_labels\n",
    "            },\n",
    "            \"average_ratio_md_emotion\": {\n",
    "                emotion: 0.0 for emotion in emotion_labels\n",
    "            },\n",
    "            \"average_ratio_md_sentiment\": {\n",
    "                sentiment: 0.0 for sentiment in sentiment_labels\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.compute_percentage_samples()\n",
    "        self.compute_average_densities_and_ratios()\n",
    "        self.normalize()\n",
    "\n",
    "    def normalize(self):\n",
    "        # Normalize\n",
    "        for key in self.stats:\n",
    "            for label in self.stats[key]:\n",
    "                self.stats[key][label] /= self.n_samples\n",
    "\n",
    "    def compute_percentage_samples(self):\n",
    "        for label_type, labels in [\n",
    "            (\"sentiment\", sentiment_labels),\n",
    "            (\"emotion\", emotion_labels),\n",
    "        ]:\n",
    "            for i in range(self.n_samples):\n",
    "                doc = self.dataset[\"nrclex_documents\"][i]\n",
    "                summ = self.dataset[\"nrclex_ref_summaries\"][i]\n",
    "                doc_unique_labels = set(doc.affect_list)\n",
    "                summ_unique_labels = set(summ.affect_list)\n",
    "\n",
    "                for label in labels:\n",
    "                    if label_type == \"emotion\":\n",
    "                        if label in doc_unique_labels:\n",
    "                            self.stats[\"percentage_documents_with_emotion\"][\n",
    "                                label\n",
    "                            ] += 1\n",
    "                        if label in summ_unique_labels:\n",
    "                            self.stats[\"percentage_summaries_with_emotion\"][\n",
    "                                label\n",
    "                            ] += 1\n",
    "                    else:\n",
    "                        if label in doc_unique_labels:\n",
    "                            self.stats[\"percentage_documents_with_sentiment\"][\n",
    "                                label\n",
    "                            ] += 1\n",
    "                        if label in summ_unique_labels:\n",
    "                            self.stats[\"percentage_summaries_with_sentiment\"][\n",
    "                                label\n",
    "                            ] += 1\n",
    "\n",
    "    def compute_average_densities_and_ratios(self):\n",
    "        for label_type, labels in [\n",
    "            (\"sentiment\", sentiment_labels),\n",
    "            (\"emotion\", emotion_labels),\n",
    "        ]:\n",
    "            state_dict = EmotionDistributionMetrics.compute_state_dict(\n",
    "                self.dataset, labels\n",
    "            )\n",
    "\n",
    "            map_names = {\n",
    "                \"ed_d\": {\n",
    "                    \"emotion\": \"average_document_emotion_density\",\n",
    "                    \"sentiment\": \"average_document_sentiment_density\",\n",
    "                },\n",
    "                \"ed_m\": {\n",
    "                    \"emotion\": \"average_summary_emotion_density\",\n",
    "                    \"sentiment\": \"average_summary_sentiment_density\",\n",
    "                },\n",
    "                \"ratio_m-d\": {\n",
    "                    \"emotion\": \"average_ratio_md_emotion\",\n",
    "                    \"sentiment\": \"average_ratio_md_sentiment\",\n",
    "                },\n",
    "            }\n",
    "            for key in map_names:\n",
    "                name_stat = map_names[key][label_type]\n",
    "                for sample in state_dict[key]:\n",
    "                    for label in sample:\n",
    "                        self.stats[name_stat][label] += sample[label]\n",
    "\n",
    "    def get_stats(self):\n",
    "        return self.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_allowed_tokens_fn(batch_id, input_ids, tok_prefixes, tokenizer):\n",
    "    position = len(input_ids)\n",
    "    if position < len(tok_prefixes[\"input_ids\"][batch_id]):\n",
    "        if (\n",
    "            tok_prefixes[\"input_ids\"][batch_id][position - 1]\n",
    "            in tokenizer.all_special_ids\n",
    "        ):\n",
    "            return None\n",
    "        return (\n",
    "            tok_prefixes[\"input_ids\"][batch_id][position - 1].view(1).tolist()\n",
    "        )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncontrolled_generation(document, tokenizer, model_name, dataset_name):\n",
    "    inputs = tokenizer(\n",
    "        [document],\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "    gen_args = copy(generation_hyperparameters[model_name][dataset_name])\n",
    "\n",
    "    gen_ids = model.generate(inputs, **gen_args)\n",
    "\n",
    "    gen_summary = [\n",
    "        tokenizer.decode(\n",
    "            g, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        .replace(\". \", \" .\\n\")\n",
    "        .replace(\"<n>\", \"\\n\")\n",
    "        for g in gen_ids\n",
    "    ][0]\n",
    "    return gen_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controlled_generation(\n",
    "    document, tokenizer, emotion_chain, model_name, dataset_name\n",
    "):\n",
    "    inputs = tokenizer(\n",
    "        [document],\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "    tok_prefix = tokenizer(\n",
    "        [emotion_chain],\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    gen_args = copy(generation_hyperparameters[model_name][dataset_name])\n",
    "\n",
    "    gen_args[\"prefix_allowed_tokens_fn\"] = partial(\n",
    "        prefix_allowed_tokens_fn, tok_prefixes=tok_prefix, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    gen_ids = model.generate(inputs, **gen_args)\n",
    "\n",
    "    gen_summary = [\n",
    "        tokenizer.decode(\n",
    "            g, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        .replace(\". \", \" .\\n\")\n",
    "        .replace(\"<n>\", \"\\n\")\n",
    "        for g in gen_ids\n",
    "    ][0]\n",
    "    return gen_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion statistics in the corpora.\n",
    "\n",
    "Statistics on emotions and sentiments in the CNNDM and XSUM corpora are considered:\n",
    "\n",
    "- The percentage of documents and summaries that have words for each emotion/sentiment.\n",
    "- Averages of emotion/sentiment densities in documents and summaries.\n",
    "- Averages of emotion/sentiment ratios M/D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_nrclex = \"./NRCLexDatasetWLemmatization/cnn_dailymail-nrclex+lemmatized.pkl\"\n",
    "xsum_nrclex = \"./NRCLexDatasetWLemmatization/xsum-nrclex+lemmatized.pkl\"\n",
    "\n",
    "with open(cnn_nrclex, \"rb\") as fr:\n",
    "    dataset = pkl.load(fr)\n",
    "\n",
    "# Repair names...\n",
    "dataset[\"nrclex_ref_summaries\"] = copy(dataset[\"nrclex_summaries\"])\n",
    "del(dataset[\"nrclex_summaries\"])\n",
    "dataset[\"nrclex_gen_summaries\"] = copy(dataset[\"nrclex_ref_summaries\"])\n",
    "\n",
    "print(json.dumps(Statistics(dataset).get_stats(), indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | Statistic     | Fear  | Anger | Anticipation | Trust | Surprise | Sadness | Disgust | Joy   | None  | Negative | Positive | None  |\n",
    "|---------|---------------|-------|-------|--------------|-------|----------|---------|---------|-------|-------|----------|----------|-------|\n",
    "| CNNDM   | % docs        | 99.66% | 98.81% | 99.58%        | 99.98% | 99.24%    | 99.67%   | 97.17%   | 99.40% | 0.0%   | 99.97%    | 97.17%    | 0.0%   |\n",
    "| CNNDM   | % summs       | 79.22% | 69.63% | 84.23%        | 90.12% | 60.36%    | 76.38%   | 53.81%   | 69.79% | 0.0%   | 91.05%    | 96.03%    | 0.0%   |\n",
    "| CNNDM   | Avg $ED_{D}$  | 3.18%  | 2.17%  | 3.52%         | 4.56%  | 1.55%     | 2.49%    | 1.23%    | 2.34%  | 78.95% | 5.43%     | 7.61%     | 86.96% |\n",
    "| CNNDM   | Avg $ED_{M}$  | 4.10%  | 2.86%  | 3.76%         | 5.00%  | 1.76%     | 3.25%    | 1.55%    | 2.55%  | 75.15% | 7.25%     | 8.44%     | 84.31% |\n",
    "| CNNDM   | Avg $R_{M/D}$ | 1.25  | 1.28  | 1.07         | 1.09  | 1.14     | 1.29    | 1.21    | 1.08  | 0.95  | 1.31     | 1.10     | 0.97  |\n",
    "| XSUM    | % docs        | 95.72% | 91.74% | 98.60%        | 99.04% | 93.58%    | 96.26%   | 85.70%   | 94.97% | 0.0%   | 98.57%    | 99.59%    | 0.0%   |\n",
    "| XSUM    | % summs       | 59.44% | 46.89% | 61.19%        | 70.52% | 38.54%    | 54.17%   | 31.75%   | 44.38% | 0.0%   | 73.96%    | 82.65%    | 0.0%   |\n",
    "| XSUM    | Avg $ED_{D}$  | 3.01%  | 2.04%  | 3.58%         | 4.54%  | 1.54%     | 2.39%    | 1.11%    | 2.17%  | 79.59% | 5.05%     | 7.69%     | 87.25% |\n",
    "| XSUM    | Avg $ED_{M}$  | 4.06%  | 2.65%  | 3.77%         | 4.88%  | 1.78%     | 3.16%    | 1.42%    | 2.32%  | 75.99% | 6.91%     | 8.48%     | 84.59% |\n",
    "| XSUM    | Avg $R_{M/D}$ | 1.34  | 1.28  | 1.09         | 1.13  | 1.18     | 1.33    | 1.15    | 1.08  | 0.95  | 1.42     | 1.15     | 0.97  |\n",
    "\n",
    "\n",
    "All documents and summaries in CNNDM and XSUM show some kind of emotion/sentiment (None=0.0 in %docs and %summs). The percentage of documents for each emotion is nearly 100% in both CNNDM and XSUM, indicating that all documents contain words that evoke all considered emotions/sentiments. There is a lower percentage of summaries than documents for each emotion/sentiment, especially in XSUM. However, all summaries convey some kind of emotion/sentiment (None=0.0), suggesting that summaries highlight only specific emotions/sentiments from the documents.\n",
    "\n",
    "The highest average density is for the emotion and sentiment None, both for documents and summaries (between 75-80% in emotions and 84-88% in sentiments). This means that the majority of words in both documents and summaries do not convey any emotion/sentiment. The top 3 emotions with the highest density in CNNDM and XSUM documents and summaries are Fear, Trust, and Anticipation, and the sentiment with the highest density is Positive in all cases. The density of emotions (for all emotions) is higher in summaries than in documents.\n",
    "\n",
    "Summaries in CNNDM and XSUM reinforce more negative emotions (fear, anger, sadness) and negative sentiments compared to others (the negative always has an average R_{M/D} above 1.20). Additionally, the averaged ratios M/D for the emotion and sentiment None are always <1, indicating that there is a higher proportion of words with emotions/sentiment in the summaries than in the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='coherenciaysesgo'></a>\n",
    "# Emotional coherence and bias\n",
    "\n",
    "We want to see:\n",
    "\n",
    "1. Are the emotions in the generated summaries coherent with the emotions in the references?\n",
    "\n",
    "2. Do the models exhibit a similar emotional bias to humans? i.e., do they reinforce [more/less] the emotions that humans reinforce [more/less] in the summaries than in the documents? From [1]:\n",
    "  > ... emotional words that appear more frequently in the human-written model summaries than in the document sets for summarizing should also be more numerous in an automatically generated summary.\n",
    "\n",
    "Two metrics:\n",
    "\n",
    "1. Correlation of emotion density $ED_M$ and $ED_G$\n",
    "\n",
    "2. Correlation of ratios $R_{M/D}$ and $R_{G/D}$\n",
    "\n",
    "<br><br>\n",
    "<img src=\"https://iili.io/RGs6Gf.png\" width=\"700\" height=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_paths = [\n",
    "    \"./AnalysisDatasetWOLemmatization/bart-large-cnn+cnn_dailymail.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/pegasus-cnn_dailymail+cnn_dailymail.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/t5-base+cnn_dailymail.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/bart-large-xsum+xsum.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/pegasus-xsum+xsum.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/t5-base+xsum.pkl\",\n",
    "]\n",
    "\n",
    "dataset_lemmatized_paths = [\n",
    "    \"./AnalysisDatasetWLemmatization/lead+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/random+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/extractive_oracle+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-large-cnn+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/pegasus-cnn_dailymail+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/t5-base+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-cnn_dailymail+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-cnn_dailymail+cnn_dailymail+lemmatized+oracle.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/lead+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/random+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/extractive_oracle+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-large-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/pegasus-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/t5-base+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized+oracle.pkl\",\n",
    "]\n",
    "\n",
    "for path in dataset_lemmatized_paths:\n",
    "    print(\"Path (model+dataset):\", path)\n",
    "\n",
    "    with open(path, \"rb\") as fr:\n",
    "        dataset = pkl.load(fr)\n",
    "        for label_type, labels in [\n",
    "            (\"sentiment\", sentiment_labels),\n",
    "            (\"emotion\", emotion_labels),\n",
    "        ]:\n",
    "            print(\"\\nLabel type:\", label_type)\n",
    "            state_dict = EmotionDistributionMetrics.compute_state_dict(\n",
    "                dataset, labels\n",
    "            )\n",
    "            print(\"\\nCorrelación entre densidades:\")\n",
    "            print(\n",
    "                \"\\nPearson:\",\n",
    "                EmotionDistributionMetrics.emotion_correlation(\n",
    "                    state_dict, labels, func=\"pearson\", mode=\"densities\"\n",
    "                ),\n",
    "            )\n",
    "            print(\n",
    "                \"\\nSpearman:\",\n",
    "                EmotionDistributionMetrics.emotion_correlation(\n",
    "                    state_dict, labels, func=\"spearman\", mode=\"densities\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\"\\n\\nCorrelación entre ratios:\")\n",
    "            print(\n",
    "                \"\\nPearson:\",\n",
    "                EmotionDistributionMetrics.emotion_correlation(\n",
    "                    state_dict, labels, func=\"pearson\", mode=\"ratios\"\n",
    "                ),\n",
    "            )\n",
    "            print(\n",
    "                \"\\nSpearman:\",\n",
    "                EmotionDistributionMetrics.emotion_correlation(\n",
    "                    state_dict, labels, func=\"spearman\", mode=\"ratios\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    print(\"\\n\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pearson correlation of emotion density $ED_M$ and $ED_G$ (Note these are Pearson correlations scaled to 0-100).\n",
    "    \n",
    "| Dataset | Model             | Fear  | Anger | Anticipation | Trust | Surprise | Sadness | Disgust | Joy   | None  | Negative | Positive | None  |\n",
    "|---------|-------------------|-------|-------|--------------|-------|----------|---------|---------|-------|-------|----------|----------|-------|\n",
    "| CNNDM   | Lead              | 64.16 | 59.69 | 38.23        | 41.71 | 37.07    | 54.72   | 48.75   | 49.82 | 51.05 | 56.85    | 45.62    | 44.08 |\n",
    "| CNNDM   | Random            | 49.49 | 44.18 | 22.56        | 27.93 | 22.81    | 35.84   | 33.06   | 34.30 | 34.50 | 40.00    | 28.75    | 27.52 |\n",
    "| CNNDM   | Extractive oracle | 76.29 | 71.67 | 56.91        | 59.80 | 55.85    | 68.29   | 65.89   | 65.97 | 66.42 | 71.31    | 61.83    | 60.60 |\n",
    "| CNNDM   | BART-CNNDM        | 64.57 | 59.33 | 39.79        | 43.12 | 39.27    | 53.89   | 50.97   | 50.20 | 50.97 | 57.33    | 45.66    | 44.32 |\n",
    "| CNNDM   | PEGASUS-CNNDM     | 63.15 | 57.99 | 40.17        | 44.15 | 39.41    | 53.41   | 49.82   | 49.79 | 51.60 | 56.34    | 45.73    | 45.13 |\n",
    "| CNNDM   | T5-BASE           | 62.41 | 57.26 | 36.39        | 40.45 | 35.27    | 51.42   | 48.05   | 45.49 | 48.19 | 55.46    | 43.14    | 42.30 |\n",
    "| CNNDM   | BART-JES          | 64.59 | 59.21 | 37.85        | 42.24 | 40.94    | 54.72   | 51.05   | 47.82 | 50.64 | 57.18    | 42.92    | 42.04 |\n",
    "| CNNDM   | BART-JES-Oracle   | 77.59 | 79.71 | 70.82        | 67.25 | 78.57    | 76.51   | 79.25   | 76.76 | 58.90 | 70.00    | 61.09    | 48.87 |\n",
    "| XSUM    | Lead              | 30.72 | 25.64 | 13.07        | 15.28 | 9.27     | 19.98   | 14.79   | 15.82 | 18.24 | 23.59    | 16.39    | 13.11 |\n",
    "| XSUM    | Random            | 28.62 | 21.80 | 8.39         | 10.56 | 10.25    | 17.88   | 11.56   | 14.08 | 16.52 | 20.60    | 11.91    | 11.91 |\n",
    "| XSUM    | Extractive oracle | 44.26 | 37.24 | 25.10        | 27.05 | 22.60    | 34.95   | 30.67   | 29.89 | 31.83 | 36.22    | 28.58    | 27.05 |\n",
    "| XSUM    | BART-XSUM         | 61.09 | 55.65 | 40.62        | 47.69 | 39.81    | 56.16   | 49.09   | 45.38 | 50.96 | 55.56    | 47.37    | 46.02 |\n",
    "| XSUM    | PEGASUS-XSUM      | 63.23 | 57.62 | 41.89        | 49.55 | 41.72    | 57.48   | 51.82   | 47.97 | 53.52 | 57.65    | 48.81    | 47.96 |\n",
    "| XSUM    | T5-BASE           | 44.36 | 35.77 | 19.74        | 21.40 | 16.90    | 33.34   | 23.45   | 24.72 | 30.61 | 35.89    | 24.18    | 24.36 |\n",
    "| XSUM    | BART-JES          | 60.88 | 53.88 | 39.25        | 45.57 | 37.61    | 55.34   | 48.50   | 44.29 | 48.08 | 54.28    | 44.39    | 42.52 |\n",
    "| XSUM    | BART-JES-Oracle   | 89.44 | 89.36 | 84.20        | 83.76 | 85.96    | 89.00   | 88.06   | 85.35 | 81.92 | 86.67    | 81.17    | 76.64 |\n",
    "\n",
    "\n",
    "* Pearson correlation of rates $R_{M/D}$ y $R_{G/D}$\n",
    "\n",
    "| Dataset | Model             | Fear  | Anger | Anticipation | Trust | Surprise | Sadness | Disgust | Joy   | None  | Negative | Positive | None  |\n",
    "|---------|-------------------|-------|-------|--------------|-------|----------|---------|---------|-------|-------|----------|----------|-------|\n",
    "| CNNDM   | Lead              | 15.09 | 16.24 | 16.19        | 15.50 | 14.64    | 19.18   | 15.23   | 17.59 | 23.21 | 20.59    | 20.01    | 21.93 |\n",
    "| CNNDM   | Random            | 0.94  | 0.00  | 1.88         | 3.33  | -0.01    | 1.55    | 1.75    | 1.60  | 1.67  | -0.01    | 0.00     | 0.15  |\n",
    "| CNNDM   | Extractive oracle | 39.03 | 37.68 | 40.38        | 42.31 | 39.59    | 41.41   | 40.16   | 41.58 | 47.82 | 44.41    | 44.55    | 45.15 |\n",
    "| CNNDM   | BART-CNNDM        | 19.76 | 19.79 | 19.94        | 20.97 | 17.01    | 21.59   | 19.62   | 20.53 | 24.82 | 20.42    | 21.77    | 23.17 |\n",
    "| CNNDM   | PEGASUS-CNNDM     | 23.01 | 23.15 | 20.11        | 24.24 | 20.34    | 22.10   | 22.44   | 22.49 | 28.77 | 24.54    | 25.12    | 26.56 |\n",
    "| CNNDM   | T5-BASE           | 17.56 | 18.22 | 16.79        | 18.74 | 14.95    | 19.40   | 18.72   | 17.03 | 22.06 | 19.97    | 20.01    | 21.86 |\n",
    "| CNNDM   | BART-JES          | 21.14 | 21.01 | 19.44        | 21.14 | 19.57    | 24.11   | 20.72   | 19.52 | 25.34 | 22.82    | 22.85    | 22.43 |\n",
    "| CNNDM   | BART-JES-Oracle   | 76.59 | 77.11 | 70.10        | 66.76 | 76.87    | 74.67   | 79.77   | 76.73 | 45.63 | 62.68    | 58.10    | 39.91 |\n",
    "| XSUM    | Lead              | 5.85  | 6.33  | 2.08         | 1.17  | 2.82     | 5.48    | 6.17    | 2.47  | 2.08  | 4.27     | 5.64     | 2.38  |\n",
    "| XSUM    | Random            | 3.95  | 2.02  | 2.38         | 2.28  | 1.79     | 3.78    | 2.52    | 3.77  | 0.90  | 2.96     | 1.08     | 1.56  |\n",
    "| XSUM    | Extractive oracle | 15.79 | 12.95 | 9.52         | 12.23 | 10.20    | 17.02   | 15.21   | 11.82 | 16.45 | 13.83    | 15.21    | 16.91 |\n",
    "| XSUM    | BART-XSUM         | 39.29 | 31.13 | 37.51        | 40.35 | 33.36    | 39.61   | 35.49   | 35.31 | 41.89 | 42.21    | 41.66    | 41.06 |\n",
    "| XSUM    | PEGASUS-XSUM      | 37.58 | 33.40 | 38.98        | 44.04 | 30.16    | 40.37   | 41.35   | 41.94 | 45.04 | 41.35    | 43.94    | 42.30 |\n",
    "| XSUM    | T5-BASE           | 9.89  | 8.54  | 5.52         | 3.48  | 4.51     | 8.59    | 4.75    | 9.15  | 9.72  | 7.98     | 7.47     | 10.68 |\n",
    "| XSUM    | BART-JES          | 38.98 | 29.54 | 35.51        | 38.22 | 26.95    | 39.52   | 36.23   | 35.13 | 37.78 | 39.81    | 40.92    | 36.41 |\n",
    "| XSUM    | BART-JES-Oracle   | 86.15 | 86.22 | 85.29        | 82.61 | 87.02    | 87.42   | 87.53   | 86.71 | 78.69 | 84.40    | 82.08    | 74.68 |\n",
    "\n",
    "The p-value is 0 in all cases, so we reject $H_0$.\n",
    "\n",
    "The models approximate emotions well in the summaries (moderate/strong correlations), but interestingly, in CNN/DM, state-of-the-art abstractive models show a correlation very similar to the baseline LEAD.\n",
    "\n",
    "They do not exhibit the same emotional bias as humans (weak correlations in the second table), i.e., they do not reinforce more/less the same emotions as humans.\n",
    "\n",
    "It is noteworthy how emotional bias is better captured in XSUM (stronger correlations than in CNNDM between the ratios). I currently don't have an explanation for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rougecorrel'></a>\n",
    "# ROUGE-emotion correlation\n",
    "\n",
    "Does ROUGE measures emotion densities and ratios?\n",
    "\n",
    "To what extent does ROUGE measure emotion densities and ratios? For this, we study the correlation between $pearson(ED_M(E_{1...N}), ED_G(E_{1...N}))$ and ROUGE (see how much ROUGE increases/reduces if the model better/worse approximates the emotion/sentiment densities of the reference); and between $pearson(R_{M/D}(E_{1...N}), R_{G/D}(E_{1...N}))$ and ROUGE (see how much ROUGE increases/reduces if the model better/worse approximates the emotional ratio of the reference).\n",
    "\n",
    "<br><br>\n",
    "<img src=\"https://iili.io/RGsDu9.png\" width=\"700\" height=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [\n",
    "    \"./AnalysisDatasetWOLemmatization/bart-large-cnn+cnn_dailymail.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/pegasus-cnn_dailymail+cnn_dailymail.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/t5-base+cnn_dailymail.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/bart-large-xsum+xsum.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/pegasus-xsum+xsum.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/t5-base+xsum.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/pegasus-xsum+xsum+lemmatized.pkl\",\n",
    "]\n",
    "\n",
    "dataset_lemmatized_paths = [\n",
    "    \"./AnalysisDatasetWLemmatization/lead+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/random+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/extractive_oracle+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-large-cnn+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/pegasus-cnn_dailymail+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/t5-base+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-cnn_dailymail+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-cnn_dailymail+cnn_dailymail+lemmatized+oracle.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/lead+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/random+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/extractive_oracle+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-large-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/pegasus-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/t5-base+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized+oracle.pkl\",\n",
    "]\n",
    "\n",
    "for path in dataset_lemmatized_paths:\n",
    "    print(\"Path (model+dataset):\", path)\n",
    "\n",
    "    with open(path, \"rb\") as fr:\n",
    "        dataset = pkl.load(fr)\n",
    "        for label_type, labels in [\n",
    "            (\"sentiment\", sentiment_labels),\n",
    "            (\"emotion\", emotion_labels),\n",
    "        ]:\n",
    "            print(\"\\nLabel type:\", label_type)\n",
    "            state_dict = EmotionDistributionMetrics.compute_state_dict(\n",
    "                dataset, labels\n",
    "            )\n",
    "            print(\"\\nCorrelación entre densidades y ROUGE:\")\n",
    "            print(\n",
    "                \"\\nPearson:\",\n",
    "                EmotionDistributionMetrics.emotion_summary_metric_correlation(\n",
    "                    state_dict,\n",
    "                    dataset,\n",
    "                    emo_metric_func=\"pearson\",\n",
    "                    mode=\"densities\",\n",
    "                    correlation_func=\"pearson\",\n",
    "                    summary_metric=\"rouge\",\n",
    "                ),\n",
    "            )\n",
    "            print(\n",
    "                \"\\nSpearman:\",\n",
    "                EmotionDistributionMetrics.emotion_summary_metric_correlation(\n",
    "                    state_dict,\n",
    "                    dataset,\n",
    "                    emo_metric_func=\"pearson\",\n",
    "                    mode=\"densities\",\n",
    "                    correlation_func=\"spearman\",\n",
    "                    summary_metric=\"rouge\",\n",
    "                ),\n",
    "            )\n",
    "            print(\"\\nCorrelación entre ratios y ROUGE:\")\n",
    "            print(\n",
    "                \"\\nPearson:\",\n",
    "                EmotionDistributionMetrics.emotion_summary_metric_correlation(\n",
    "                    state_dict,\n",
    "                    dataset,\n",
    "                    emo_metric_func=\"pearson\",\n",
    "                    mode=\"ratios\",\n",
    "                    correlation_func=\"pearson\",\n",
    "                    summary_metric=\"rouge\",\n",
    "                ),\n",
    "            )\n",
    "            print(\n",
    "                \"\\nSpearman:\",\n",
    "                EmotionDistributionMetrics.emotion_summary_metric_correlation(\n",
    "                    state_dict,\n",
    "                    dataset,\n",
    "                    emo_metric_func=\"pearson\",\n",
    "                    mode=\"ratios\",\n",
    "                    correlation_func=\"spearman\",\n",
    "                    summary_metric=\"rouge\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    print(\"\\n\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Correlation between pearson($ED_M$, $ED_G$) and ROUGE (\"if it improves/worsens capturing emotion density, does ROUGE improve/worsen?\")\n",
    "\n",
    "| Dataset | Model             | Emotion | Sentiment |\n",
    "|---------|-------------------|---------|-----------|\n",
    "| CNNDM   | Lead              | 25.97   | 12.67     |\n",
    "| CNNDM   | Random            | 24.10   | 17.07     |\n",
    "| CNNDM   | Extractive oracle | 27.80   | 18.97     |\n",
    "| CNNDM   | BART-CNNDM        | 28.83   | 19.73     |\n",
    "| CNNDM   | PEGASUS-CNNDM     | 29.92   | 20.44     |\n",
    "| CNNDM   | T5-BASE           | 27.57   | 17.48     |\n",
    "| CNNDM   | BART-JES          | 25.06   | 17.11     |\n",
    "| CNNDM   | BART-JES-Oracle   | 39.12   | 28.50     |\n",
    "| XSUM    | Lead              | 10.90   | 8.71      |\n",
    "| XSUM    | Random            | 13.47   | 11.97     |\n",
    "| XSUM    | Extractive oracle | 9.26    | 7.89      |\n",
    "| XSUM    | BART-XSUM         | 21.55   | 12.76     |\n",
    "| XSUM    | PEGASUS-XSUM      | 22,48   | 13.31     |\n",
    "| XSUM    | T5-BASE           | 11.93   | 9.66      |\n",
    "| XSUM    | BART-JES          | 17.94   | 11.73     |\n",
    "| XSUM    | BART-JES-Oracle   | 16.51   | 10.25     |\n",
    "\n",
    "* Correlation between pearson($R_{M/D}$, $R_{G/D}$) and ROUGE (\"if it improves/worsens capturing emotional bias, does ROUGE improve/worsen?\")\n",
    "\n",
    "| Dataset | Model             | Emotion | Sentiment |\n",
    "|---------|-------------------|---------|-----------|\n",
    "| CNNDM   | Lead              | 27.12   | 17.79     |\n",
    "| CNNDM   | Random            | 21.03   | 12.92     |\n",
    "| CNNDM   | Extractive oracle | 31.21   | 15.35     |\n",
    "| CNNDM   | BART-CNNDM        | 31.87   | 16.90     |\n",
    "| CNNDM   | PEGASUS-CNNDM     | 30.89   | 17.92     |\n",
    "| CNNDM   | T5-BASE           | 30.39   | 16.91     |\n",
    "| CNNDM   | BART-JES          | 28.31   | 18.22     |\n",
    "| CNNDM   | BART-JES-Oracle   | 64.44   | 58.29     |\n",
    "| XSUM    | Lead              | 10.69   | 4.16      |\n",
    "| XSUM    | Random            | 13.02   | 8.47      |\n",
    "| XSUM    | Extractive oracle | 15.24   | 8.66      |\n",
    "| XSUM    | BART-XSUM         | 34.20   | 21.08     |\n",
    "| XSUM    | PEGASUS-XSUM      | 35.09   | 21.62     |\n",
    "| XSUM    | T5-BASE           | 12.06   | 8.74      |\n",
    "| XSUM    | BART-JES          | 31.05   | 20.77     |\n",
    "| XSUM    | BART-JES-Oracle   | 27.36   | 23.45     |\n",
    "\n",
    "Positive correlation in all cases, if emotion/sentiment densities and ratios are better approximated, ROUGE increases. However, the correlation is weak in almost all cases. In any case, emotions correlate with ROUGE more than sentiment. Emotional bias (emotion ratios) seems to correlate better than density, especially in XSUM. In some models, the correlation in XSUM increases by more than 10 points for BART-XSUM and PEGASUS-XSUM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='alucinacion'></a>\n",
    "# Emotions of novel words\n",
    "\n",
    "This section explores whether the emotions of the novel words in the generated summaries are 'unfaithful' with respect to the emotions in the reference summaries.\n",
    "\n",
    "<img src=\"https://iili.io/RGsAhu.png\" width=\"1000\" height=\"1000\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [\n",
    "    \"./AnalysisDatasetWOLemmatization/bart-large-cnn+cnn_dailymail.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/pegasus-cnn_dailymail+cnn_dailymail.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/t5-base+cnn_dailymail.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/bart-large-xsum+xsum.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/pegasus-xsum+xsum.pkl\",\n",
    "    \"./AnalysisDatasetWOLemmatization/t5-base+xsum.pkl\",\n",
    "]\n",
    "\n",
    "\n",
    "dataset_lemmatized_paths = [\n",
    "    \"./AnalysisDatasetWLemmatization/bart-large-cnn+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/pegasus-cnn_dailymail+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/t5-base+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-cnn_dailymail+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-cnn_dailymail+cnn_dailymail+lemmatized+oracle.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-large-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/pegasus-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/t5-base+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized+oracle.pkl\",\n",
    "]\n",
    "\n",
    "\n",
    "for path in dataset_lemmatized_paths:\n",
    "    print(\"Path (model+dataset):\", path)\n",
    "\n",
    "    with open(path, \"rb\") as fr:\n",
    "        dataset = pkl.load(fr)\n",
    "        print(EmotionalHallucinationMetric.compute(dataset, lemmatized=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | Model           | Precision      | Recall         | $F_1$          |\n",
    "|---------|-----------------|----------------|----------------|----------------|\n",
    "| CNNDM   | BART-CNNDM      | 89.54$\\pm$1.28 | 37.07$\\pm$1.32 | 48.05$\\pm$1.29 |\n",
    "| CNNDM   | PEGASUS-CNNDM   | 90.04$\\pm$0.98 | 34.71$\\pm$0.97 | 45.89$\\pm$0.97 |\n",
    "| CNNDM   | T5-BASE         | 89.82$\\pm$1.11 | 30.68$\\pm$1.01 | 41.87$\\pm$1.35 |\n",
    "| CNNDM   | BART-JES        | 89.46$\\pm$0.90 | 33.71$\\pm$0.86 | 44.81$\\pm$0.85 |\n",
    "| CNNDM   | BART-JES-Oracle | 98.85$\\pm$0.16 | 51.63$\\pm$0.55 | 63.60$\\pm$0.49 |\n",
    "| XSUM    | BART-XSUM       | 79.59$\\pm$0.67 | 49.85$\\pm$0.66 | 55.57$\\pm$0.58 |\n",
    "| XSUM    | PEGASUS-XSUM    | 81.23$\\pm$0.64 | 50.82$\\pm$0.65 | 56.91$\\pm$0.58 |\n",
    "| XSUM    | T5-BASE         | 73.08$\\pm$1.89 | 35.08$\\pm$1.44 | 41.87$\\pm$1.35 |\n",
    "| XSUM    | BART-JES        | 79.66$\\pm$0.67 | 50.64$\\pm$0.67 | 56.13$\\pm$0.59 |\n",
    "| XSUM    | BART-JES-Oracle | 94.36$\\pm$0.34 | 70.38$\\pm$0.56 | 76.44$\\pm$0.46 |\n",
    "\n",
    "In general, they seem to hallucinate very little. Most of the novel words generated by the models have emotions present in the reference (high Precision). It is logical that the Recall is not very high since it is challenging to cover all emotions with novel words alone, but still, the values do not seem bad. In the worst case in the table, more than one-third of the emotions in the reference (Recall 34.71) are covered by novel words. In XSUM, the models hallucinate more (lower precision), but they cover more emotions from the reference by generating more novel words (higher recall). This boosts the $F_1$ compared to CNNDM (recall difference greater than precision difference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE y BERTScore of summarization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lemmatized_paths = [\n",
    "    \"./AnalysisDatasetWLemmatization/lead+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/random+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/extractive_oracle+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-large-cnn+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/pegasus-cnn_dailymail+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/t5-base+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-cnn_dailymail+cnn_dailymail+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-cnn_dailymail+cnn_dailymail+lemmatized+oracle.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/lead+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/random+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/extractive_oracle+xsum+lemmatized.pkl\",    \n",
    "    \"./AnalysisDatasetWLemmatization/bart-large-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/pegasus-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/t5-base+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized.pkl\",\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized+oracle.pkl\",\n",
    "]\n",
    "\n",
    "\n",
    "for path in dataset_lemmatized_paths:\n",
    "    print(\"Path (model+dataset):\", path)\n",
    "\n",
    "    with open(path, \"rb\") as fr:\n",
    "        dataset = pkl.load(fr)\n",
    "        print(\"ROUGE-1:\", np.array(dataset[\"rouge_scores\"][\"rouge1_f1\"]).mean())\n",
    "        print(\"ROUGE-2:\", np.array(dataset[\"rouge_scores\"][\"rouge2_f1\"]).mean())\n",
    "        print(\"ROUGE-L:\", np.array(dataset[\"rouge_scores\"][\"rougeLsum_f1\"]).mean())\n",
    "        print(\"BertScore:\", np.array(dataset[\"bert_scores\"][\"f1\"]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | Model         | R1    | R2    | RL    | BS    |\n",
    "|---------|---------------|-------|-------|-------|-------|\n",
    "| CNNDM   | Lead | 40.05  | 17.48  | 36.34  | 23.45  |\n",
    "| CNNDM   | Random | 28.48  | 8.34  | 25.51  | 11.88  |\n",
    "| CNNDM   | Extractive oracle | 52.34  | 30.23 | 48.86 | 39.77  |\n",
    "| CNNDM   | BART-CNNDM    | 43.76 | 20.86 | 40.68 | 33.64 |\n",
    "| CNNDM   | PEGASUS-CNNDM | 43.96 | 21.38 | 41.07 | 35.18 |\n",
    "| CNNDM   | T5-BASE | 39.95  | 17.47 | 36.93 | 22.93  |\n",
    "| CNNDM   | BART-JES | 42.04  | 19.19 | 39.09 | 31.24  |\n",
    "| CNNDM   | BART-JES-Oracle | 48.60  | 28.45 | 45.98 | 24.30  |\n",
    "| XSUM   | Lead | 16.71  | 1.65  | 12.30  | 14.27  |\n",
    "| XSUM   | Random | 15.23  | 1.77  | 11.38  | 11.71  |\n",
    "| XSUM   | Extractive oracle | 29.38  | 8.68 | 22.43 | 22.66  |\n",
    "| XSUM    | BART-XSUM     | 45.23 | 22.13 | 37.02 | 50.13 |\n",
    "| XSUM    | PEGASUS-XSUM  | 47.16 | 24.58 | 39.31 | 52.74 |\n",
    "| XSUM   | T5-BASE | 20.68 | 3.18 | 16.36 | 8.08 |\n",
    "| XSUM   | BART-JES | 42.35  | 19.45 | 34.76  | 48.02   |\n",
    "| XSUM   | BART-JES-Oracle | 58.63  | 34.06 | 51.36 | 58.88   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modelado'></a>\n",
    "# Possible improvements in modeling.\n",
    "\n",
    "While the models already appear to be quite good, are there ways to improve the results of the previous metrics (correlating more with densities and ratios of emotions in the references - especially the ratio - and reducing emotional hallucination -increasing precision, recall, and $F_1$-) in modeling?\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "* **Emotional loss**: <br>\n",
    "<img src=\"https://iili.io/RGsIp9.png\" width=\"1000\" height=\"1000\"/>\n",
    "\n",
    "* **Join Emotion and Summary generation**: Similar to [Planning with Entity Chains for Abstractive Summarization](https://arxiv.org/abs/2104.07606). Train a model to generate, as a prefix, the sequence of words with emotions from the reference (using lemmas to avoid restricting to exact words), followed by the reference summary. For example, given a document, generate -> <em>[chain] afraid | fright | sad ||| happy [summary] I'm afraid, frightened, and sad. No, I'm happy!</em>. This approach forces the model to create a plan with emotional words to use in the summary generation (the summary is conditioned on the plan/prefix and the document). This approach has the advantage that, during inference, the generation can be controlled by providing the emotional word sequence to condition the summary generation.\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> We briefly experimented with \"Join Emotion and Summary generation\" using BART as backbone (BART-JES) and its oracle (BART-JES-Oracle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='controlled'></a>\n",
    "# Generation with Emotion-Conditioned BART-JES\n",
    "\n",
    "\n",
    "In BART-JES, the summary is conditioned by a chain of emotional words. We can set this chain in inference as a prefix to control the generation (of course, the prefix can later be removed if we only want the generated summary). The following cells illustrate how to do this with a sample from the XSUM corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/jogonba2/.cache/huggingface/datasets/xsum/default/1.2.0/f9abaabb5e2b2a1e765c25417264722d31877b34ec34b437c53242f6e5c30d6d)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"xsum\"\n",
    "doc_summ_keys = {\n",
    "    \"document\": \"document\",\n",
    "    \"summary\": \"summary\",\n",
    "}\n",
    "version = None\n",
    "\n",
    "dataset = load_dataset(dataset_name, version=version)\n",
    "\n",
    "documents = [doc for doc in dataset[\"test\"][doc_summ_keys[\"document\"]]]\n",
    "ref_summaries = [\n",
    "    ref_summ for ref_summ in dataset[\"test\"][doc_summ_keys[\"summary\"]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"jogonba2/bart-JES-xsum\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_gen_args = copy(generation_hyperparameters[model_name][dataset_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Reinforcing Negative Emotion in a Sample with Negative Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<u><h4>Document</h4></u></br></br>Sexist, racist and homophobic remarks were being used by pupils against school staff, as well as offensive comments about appearance, the NASUWT said.\n",
       "There were also examples of parents being abusive on social media, it added.\n",
       "About 60% of 1,500 teachers questioned in a poll said they had faced abuse, compared with 21% last year.\n",
       "In one case, a photograph of a teacher was posted online with an insulting word underneath.\n",
       "In another, pupils used the name of a heavily pregnant school worker to post insults, the teachers' union said.\n",
       "Of those who had been subjected to insults, nearly half (48%) said these remarks were posted by pupils, 40% said they were put up by parents, and 12% said both parents and pupils were responsible.\n",
       "Almost two-thirds (62%) said pupils had posted insulting comments, while just over a third (34%) said students had taken photos or videos without consent.\n",
       "A third (33%) received remarks about their performance as a teacher, 9% had faced allegations from pupils about inappropriate behaviour and 8% had been subjected to threatening behaviour.\n",
       "More than half (57%) of pupils responsible were aged between 14 and 16, and 38% were 11 to 14, the teachers' poll found, with a fifth aged 16 to 19 and 5% were seven to 11.\n",
       "Among the examples published by NASUWT was the case of a student uploading a teacher's photo and then, along with classmates, writing insults underneath.\n",
       "One teacher had been harassed for nine months by students who sent sexually explicit messages and set up a fake social media account in the teacher's name.\n",
       "The union said it had been told of a teacher receiving the comment \"I hope she gets cancer\", while the heavily pregnant worker had faced abusive remarks.\n",
       "Another school worker faced comments from a pupil's family member about how they looked and that they were ugly.\n",
       "Chris Keates, the union's general secretary, said: \"It is deeply worrying to see that the abuse of teachers has risen by such a huge margin this year.\n",
       "\"Equally concerning is that it appears that more parents are the perpetrators of the abuse.\n",
       "\"The vile, insulting and personal comments are taking their toll on teachers' health and well-being, and undermining their confidence to do their job.\""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<u><h4>Reference summary</h4></u></br></br>More teachers are facing abuse on social media, warns a teachers' union."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<u><h4>Generated summary (uncontrolled)</h4></u></br></br>[emotions] abuse | teacher | rise | survey | suggest [summary] Abuse of teachers in England has risen by a third in the past year, a survey suggests."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<u><h4>Generated summary (controlled <font size='2'>using the chain '[emotions] mediocre | bully | harm [summary]'</font>)</h4></u></br></br>[emotions] mediocre | bully | harm [summary] Misogynistic and homophobic bullying by pupils in England is harming teachers' health and well-being, a union has said."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_example = 110\n",
    "\n",
    "document, ref_summary = documents[idx_example], ref_summaries[idx_example]\n",
    "\n",
    "uncontrolled_summary = uncontrolled_generation(\n",
    "    document, tokenizer, model_name, dataset_name\n",
    ")\n",
    "\n",
    "emotion_chain = \"[emotions] mediocre | bully | harm [summary]\"\n",
    "controlled_summary = controlled_generation(\n",
    "    document, tokenizer, emotion_chain, model_name, dataset_name\n",
    ")\n",
    "\n",
    "display(HTML(\"<u><h4>Document</h4></u></br></br>%s\" % document))\n",
    "\n",
    "display(HTML(\"<u><h4>Reference summary</h4></u></br></br>%s\" % ref_summary))\n",
    "\n",
    "display(\n",
    "    HTML(\"<u><h4>Generated summary (uncontrolled)</h4></u></br></br>%s\" % uncontrolled_summary)\n",
    ")\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        \"<u><h4>Generated summary (controlled <font size='2'>using the chain '%s'</font>)</h4></u></br></br>%s\"\n",
    "        % (emotion_chain, controlled_summary)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Reinforcing Positive Emotions in a Sample with Negative Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<u><h4>Document</h4></u></br></br>The former London Irish man, 29, who can play in the second or back row, was injured in Friday's loss at Newcastle.\n",
       "\"He really hurt his neck and he has got pins and needles in his arms and through his hands,\" director of rugby Todd Blackadder told BBC Points West.\n",
       "\"So he could be out for a little while yet but it's hopefully not too serious. But to lose a big boy like that is an opportunity for someone else.\""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<u><h4>Reference summary</h4></u></br></br>Bath forward Matt Garvey is to have surgery after sustaining a neck injury."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<u><h4>Generated summary (uncontrolled)</h4></u></br></br>[emotions] prop | miss | rest | season | suffer | injury [summary] Bristol prop Alex Goodfellow could miss the rest of the season after suffering a neck injury."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<u><h4>Generated summary (controlled <font size='2'>using the chain '[emotions] love | hope [summary]'</font>)</h4></u></br></br>[emotions] love | hope [summary] Bath's much-loved scrum-half Sean O'Brien is hoping to be out for \"a little while\" with a broken neck."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_example = 115\n",
    "\n",
    "document, ref_summary = documents[idx_example], ref_summaries[idx_example]\n",
    "\n",
    "\n",
    "uncontrolled_summary = uncontrolled_generation(\n",
    "    document, tokenizer, model_name, dataset_name\n",
    ")\n",
    "\n",
    "emotion_chain = \"[emotions] love | hope [summary]\"\n",
    "controlled_summary = controlled_generation(\n",
    "    document, tokenizer, emotion_chain, model_name, dataset_name\n",
    ")\n",
    "\n",
    "display(HTML(\"<u><h4>Document</h4></u></br></br>%s\" % document))\n",
    "\n",
    "display(HTML(\"<u><h4>Reference summary</h4></u></br></br>%s\" % ref_summary))\n",
    "\n",
    "\n",
    "display(\n",
    "    HTML(\"<u><h4>Generated summary (uncontrolled)</h4></u></br></br>%s\" % uncontrolled_summary)\n",
    ")\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        \"<u><h4>Generated summary (controlled <font size='2'>using the chain '%s'</font>)</h4></u></br></br>%s\"\n",
    "        % (emotion_chain, controlled_summary)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualizaciones'></a>\n",
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples with Lower and Higher Emotional Hallucination for a Specific Corpus and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jogonba2/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./AnalysisDatasetWOLemmatization/pegasus-xsum+xsum.pkl\"\n",
    "dataset_lemmatized_path = \"./AnalysisDatasetWLemmatization/bart-large-cnn+cnn_dailymail+lemmatized.pkl\"\n",
    "dataset_lemmatized_path = (\n",
    "    \"./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized+oracle.pkl\"\n",
    ")\n",
    "lemmatized = True\n",
    "\n",
    "with open(dataset_lemmatized_path if lemmatized else dataset_path, \"rb\") as fr:\n",
    "    dataset = pkl.load(fr)\n",
    "    outputs = Visualization.get_top_k_examples_emotional_hallucination(\n",
    "        dataset,\n",
    "        k=5000,\n",
    "        reverse=True,\n",
    "        sort_metric=\"f1\",\n",
    "        lemmatized=lemmatized,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3015 with ./AnalysisDatasetWLemmatization/bart-JES-xsum+xsum+lemmatized+oracle.pkl\n",
      "--------------------------------------------------\n",
      "\n",
      "Precision: 1.0\n",
      "Recall: 0.8888888888888888\n",
      "F1: 0.9411764705882353\n",
      "\n",
      "\n",
      "Document: The 25-year-old from Plymouth, who also dived at the 2006 Commonwealth Games and 2009 World Championship, has quit after a\n",
      "\"I just feel like my body is telling me to give up,\" the 10m platform diver told BBC South West Sport.\n",
      "\"I'm 25-years-old and knowing there's another four years until Rio, I just don't think my body will hold out.\"\n",
      "My dream, even as a little girl, was to make an Olympic Games. Not making London was heartbreaking and my dreams were shattered\n",
      "Graddon, who won a bronze medal at the 2009 European Diving Championship in Turin, cites 2008 as the best year of her career as a diver.\n",
      "She said: \"I'd come back from injury and illness and that year I was British champion and qualified for the Olympic place for Great Britain, although I didn't make Beijing.\"\n",
      "Graddon, who trained alongside Tom Daley and Tonia Couch in Plymouth, says her biggest regret is never being selected for an Olympics, after also missing out on a place at London 2012.\n",
      "\"My dream, even as a little girl, was to make an Olympic Games,\" she added.\n",
      "\"Not making Beijing was sad and I was gutted not to make it, but I thought 'head down, London's in four years'.\n",
      "\"But not making London was heartbreaking and my dreams were shattered.\n",
      "\"I knew I probably wouldn't make Rio, so it was horrible and the worst feeling not getting what I wanted, but that's sport.\"\n",
      "\n",
      "\n",
      "Ref summary: European bronze-medal winning diver Brooke Graddon has announced her retirement from the sport.\n",
      "\n",
      "\n",
      "Gen summary: British Olympic and European medal-winning diver Rebecca Graddon has announced her retirement from diving.\n",
      "\n",
      "\n",
      "Emotion lemmas in ref summary: {'medal': ['anticipation', 'joy', 'positive', 'surprise', 'trust'], 'win': ['anticipation', 'disgust', 'joy', 'positive', 'sadness', 'surprise', 'trust'], 'retirement': ['anticipation', 'fear', 'joy', 'negative', 'positive', 'sadness', 'trust']}\n",
      "\n",
      "\n",
      "Emotion lemmas in gen summary: {'medal': ['anticipation', 'joy', 'positive', 'surprise', 'trust'], 'win': ['anticipation', 'disgust', 'joy', 'positive', 'sadness', 'surprise', 'trust'], 'retirement': ['anticipation', 'fear', 'joy', 'negative', 'positive', 'sadness', 'trust']}\n",
      "\n",
      "\n",
      "Lemmas of the novel words with emotions (the whole word in the gen summary does not appear in the document): [('retirement', ['anticipation', 'fear', 'joy', 'negative', 'positive', 'sadness', 'trust']), ('medal', ['anticipation', 'joy', 'positive', 'surprise', 'trust'])]\n"
     ]
    }
   ],
   "source": [
    "c = 3015\n",
    "\n",
    "print(\"Example %d with %s\" % (c, dataset_lemmatized_path))\n",
    "print(\"-\" * 50 + \"\\n\")\n",
    "print(\"Precision:\", outputs[\"precision\"][c])\n",
    "print(\"Recall:\", outputs[\"recall\"][c])\n",
    "print(\"F1:\", outputs[\"f1\"][c])\n",
    "\n",
    "print(\"\\n\\nDocument:\", outputs[\"documents\"][c])\n",
    "print(\"\\n\\nRef summary:\", outputs[\"ref_summaries\"][c])\n",
    "print(\"\\n\\nGen summary:\", outputs[\"gen_summaries\"][c])\n",
    "\n",
    "if not lemmatized:\n",
    "    print(\n",
    "        \"\\n\\nEmotion words in ref summary:\",\n",
    "        outputs[\"nrclex_ref_summaries\"][c].affect_dict,\n",
    "    )\n",
    "    print(\n",
    "        \"\\n\\nEmotion words in gen summary:\",\n",
    "        outputs[\"nrclex_gen_summaries\"][c].affect_dict,\n",
    "    )\n",
    "    print(\n",
    "        \"\\n\\nNovel words with emotions:\",\n",
    "        outputs[\"novel_words_with_emotions\"][c],\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"\\n\\nEmotion lemmas in ref summary:\",\n",
    "        outputs[\"nrclex_ref_summaries\"][c].affect_dict,\n",
    "    )\n",
    "    print(\n",
    "        \"\\n\\nEmotion lemmas in gen summary:\",\n",
    "        outputs[\"nrclex_gen_summaries\"][c].affect_dict,\n",
    "    )\n",
    "    print(\n",
    "        \"\\n\\nLemmas of the novel words with emotions (the whole word in the gen summary does not appear in the document):\",\n",
    "        outputs[\"novel_words_with_emotions\"][c],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./AnalysisDatasetWOLemmatization/bart-large-xsum+xsum.pkl\"\n",
    "dataset_lemmatized_path = \"./AnalysisDatasetWLemmatization/bart-large-cnn+cnn_dailymail+lemmatized.pkl\"\n",
    "\n",
    "with open(dataset_lemmatized_path, \"rb\") as fr:\n",
    "    dataset = pkl.load(fr)\n",
    "    state_dict = EmotionDistributionMetrics.compute_state_dict(\n",
    "        dataset, emotion_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 1338 con ./AnalysisDatasetWLemmatization/bart-large-cnn+cnn_dailymail+lemmatized.pkl\n",
      "--------------------------------------------------\n",
      "\n",
      "ROUGEs F1 (1-2-L): 0.46280991735537186 0.20168067226890757 0.4297520661157025\n",
      "\n",
      "\n",
      "Documento: A game of cat and mouse has been captured in a series of striking images as the pair battle it out on a shed rooftop like a real life version of much-loved cartoon duo Tom and Jerry. It is an age-old rivalry that rarely ends well for one of its parties and so it proved in this remarkable set of photos. The snaps of a cat playing with a mouse on a roof in Shepton Mallet, Somerset, illustrate the perils the tiny rodents face in the town. Ironically the pet cat's name is Mouse. Unfortunately for this mouse that's where all similarities between the moggy and its namesakes end. The pictures were taken by the cat's owner Jason Bryant who confirmed the inevitable outcome of the encounter. 'My cat is a very good mouser,' he said. 'She's done it before. She often brings them into the house at 3am and then I can rescue them. But she took this poor little thing on to the shed roof where I couldn't reach it.' Ready to pounce: The mouse flies through the air as its feline nemesis looks to bring it back down to earth with a bang . Eeek: A mouse tries to escape the clutches of a pet cat named Mouse as the pair engage in the age-old game of cat and mouse . In flight: The mouse, unable to evade its moggy hunter, is thrown into the air on the roof of a shed in Shepton Mallet in Somerset . Somersaulting away: The snaps  illustrate the perils that tiny rodents face in the town with cats like Mouse around . Cartoon characters: The pair battle it out on a rooftop like a real life version of much-loved cartoon duo Tom and Jerry . Let's play: The pictures were taken by the cat's owner Jason Bryant who confirmed the inevitable outcome of the encounter . Reaching out: The mouse is flung high in the sky as it finally admits defeat in the game of cat and mouse .\n",
      "\n",
      "\n",
      "Referencia: Age-old  game of cat and mouse is brought to life in these quirky pictures taken in Shepton Mallet in Somerset .\n",
      "The pair are seen battling it out on the roof of a shed in a real life take on an episode of Tom and Jerry .\n",
      "Ironically, the cat's name is Mouse. The pictures show the dangers small rodents have to be aware of in the area .\n",
      "\n",
      "\n",
      "Generado: The snaps of a cat playing with a mouse on a roof in Shepton Mallet, Somerset, illustrate the perils the tiny rodents face in the town .\n",
      "Ironically the pet cat's name is Mouse .\n",
      "The pictures were taken by the cat's owner Jason Bryant who confirmed the inevitable outcome of the encounter.\n",
      "\n",
      "\n",
      "ED del documento: {'fear': 0.02077922077922078, 'anger': 0.012987012987012988, 'anticipation': 0.025974025974025976, 'trust': 0.033766233766233764, 'surprise': 0.015584415584415584, 'sadness': 0.01818181818181818, 'disgust': 0.012987012987012988, 'joy': 0.02857142857142857, 'none': 0.8311688311688312}\n",
      "\n",
      "\n",
      "ED de la referencia: {'fear': 0.014492753623188406, 'anger': 0.014492753623188406, 'anticipation': 0.0, 'trust': 0.028985507246376812, 'surprise': 0.0, 'sadness': 0.028985507246376812, 'disgust': 0.0, 'joy': 0.0, 'none': 0.9130434782608695}\n",
      "\n",
      "\n",
      "ED del generado: {'fear': 0.01818181818181818, 'anger': 0.0, 'anticipation': 0.01818181818181818, 'trust': 0.01818181818181818, 'surprise': 0.0, 'sadness': 0.01818181818181818, 'disgust': 0.0, 'joy': 0.0, 'none': 0.9272727272727272}\n",
      "\n",
      "\n",
      "R{M/D}: {'fear': 0.697463768115942, 'anger': 1.115942028985507, 'anticipation': 0.0, 'trust': 0.8584169453734672, 'surprise': 0.0, 'sadness': 1.5942028985507248, 'disgust': 0.0, 'joy': 0.0, 'none': 1.0985054347826086}\n",
      "\n",
      "\n",
      "R(G/D}): {'fear': 0.875, 'anger': 0.0, 'anticipation': 0.7, 'trust': 0.5384615384615384, 'surprise': 0.0, 'sadness': 1.0, 'disgust': 0.0, 'joy': 0.0, 'none': 1.1156249999999999}\n",
      "\n",
      "\n",
      "affect_dict de la referencia: {'old': ['sadness'], 'battle': ['anger', 'negative'], 'shed': ['negative'], 'real': ['positive', 'trust'], 'show': ['trust'], 'danger': ['fear', 'negative', 'sadness'], 'small': ['negative']}\n",
      "\n",
      "\n",
      "affect_dict del generado: {'illustrate': ['positive'], 'peril': ['anticipation', 'fear', 'negative', 'sadness'], 'pet': ['negative'], 'confirm': ['positive', 'trust'], 'outcome': ['positive']}\n",
      "\n",
      "\n",
      "Emociones en la referencia: {'trust', 'positive', 'anger', 'fear', 'sadness', 'negative'}\n",
      "\n",
      "\n",
      "Emociones en el generado: {'trust', 'positive', 'fear', 'anticipation', 'sadness', 'negative'}\n"
     ]
    }
   ],
   "source": [
    "c = 1338\n",
    "\n",
    "print(\"Ejemplo %d con %s\" % (c, dataset_lemmatized_path))\n",
    "print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\n",
    "    \"ROUGEs F1 (1-2-L):\",\n",
    "    dataset[\"rouge_scores\"][\"rouge1_f1\"][c],\n",
    "    dataset[\"rouge_scores\"][\"rouge2_f1\"][c],\n",
    "    dataset[\"rouge_scores\"][\"rougeLsum_f1\"][c],\n",
    ")\n",
    "print(\"\\n\\nDocumento:\", dataset[\"documents\"][c])\n",
    "print(\"\\n\\nReferencia:\", dataset[\"ref_summaries\"][c])\n",
    "print(\"\\n\\nGenerado:\", dataset[\"gen_summaries\"][c])\n",
    "print(\"\\n\\nED del documento:\", state_dict[\"ed_d\"][c])\n",
    "print(\"\\n\\nED de la referencia:\", state_dict[\"ed_m\"][c])\n",
    "print(\"\\n\\nED del generado:\", state_dict[\"ed_g\"][c])\n",
    "print(\"\\n\\nR{M/D}:\", state_dict[\"ratio_m-d\"][c])\n",
    "print(\"\\n\\nR(G/D}):\", state_dict[\"ratio_g-d\"][c])\n",
    "print(\n",
    "    \"\\n\\naffect_dict de la referencia:\",\n",
    "    dataset[\"nrclex_ref_summaries\"][c].affect_dict,\n",
    ")\n",
    "print(\n",
    "    \"\\n\\naffect_dict del generado:\",\n",
    "    dataset[\"nrclex_gen_summaries\"][c].affect_dict,\n",
    ")\n",
    "print(\n",
    "    \"\\n\\nEmociones en la referencia:\",\n",
    "    set(dataset[\"nrclex_ref_summaries\"][c].affect_list),\n",
    ")\n",
    "print(\n",
    "    \"\\n\\nEmociones en el generado:\",\n",
    "    set(dataset[\"nrclex_gen_summaries\"][c].affect_list),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
